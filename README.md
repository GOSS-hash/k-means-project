# House Grouping System - K-Means Clustering and Random Forest Classification
This project employs K-Means clustering to classify houses based on their geographic location and median income, using the California Housing dataset derived from the 1990 census data. It integrates unsupervised learning (K-Means) for clustering houses into groups and supervised learning (Random Forest) to predict these groups for new data.

## Dataset Overview
The California Housing dataset contains data from the 1990 census and includes features like:
- Latitude and Longitude for geographic location
- Median Income (MedInc)

The data was split into training and testing sets for model training and validation. Clusters are used to categorize houses based on location and income.

## Model Training and Evaluation
### K-Means Clustering
- Initialized K-Means with 6 clusters.
- Fitted the model on training data and assigned cluster labels.
- Clusters visualized using scatter plots for both training and test data.

### Random Forest Classifier
- Trained a Random Forest classifier to predict the cluster labels.
- Evaluated model performance using accuracy and classification report metrics.

### Results Summary
- K-Means successfully grouped houses into distinct clusters.
- Random Forest classifier achieved high accuracy, indicating effective learning from the cluster labels generated by K-Means.

## Usage
To replicate the analysis:
- Load the dataset.
- Run the K-Means clustering.
- Train the Random Forest classifier on cluster labels.
- Predict and visualize the results.

For more detailed steps and code execution, see the provided Jupyter notebooks.

## Contact
For inquiries or contributions, please contact [spomar36@gmail.com](mailto:spomar36@gmail.com).

## Production Services
To ensure our house grouping system operates seamlessly in a production environment, we leverage the following cloud services and ML operations practices:

### AWS
- **Amazon S3**: Store and retrieve any amount of data at any time.
- **AWS Lambda**: Run code in response to data changes in S3 buckets.
- **Amazon SageMaker**: Train, tune, and deploy machine learning models.

### Azure
- **Azure Blob Storage**: For large-scale data storage.
- **Azure Functions**: Event-driven compute for data processing.
- **Azure Machine Learning Service**: Build, train, and deploy models.

### GCP
- **Google Cloud Storage**: Durable and scalable object storage.
- **Cloud Functions**: Serverless execution of functions in response to cloud events.
- **AI Platform**: End-to-end machine learning model lifecycle management.

### ML Operations
- **Continuous Training**: Implement pipelines to retrain models with new data.
- **Data Validation**: Use services like AWS Data Pipeline or Azure Data Factory for data integrity checks.
- **Model Monitoring**: Track model performance and data drift to maintain accuracy over time.

These services ensure our model stays updated, scalable, and maintainable, with robust ML operations that support continuous improvement.

## Quickstart

```bash
git clone https://github.com/GOSS-hash/k-means-project

